{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# bert-large-cased-whole-word-masking-finetuned-squad μοντέλο και αυτόματο tokenization. Θεωρώ hallucinations τα datapoints στα οποία p(Hallucination) = 1. Threshold μέσω μέσου όρου των συνημιτόνων των γνωστών hallucinations. Προκύπτουν 32 πιθανά hallucinations εκ των οποίων το 46.875% είναι σωστό.\n",
        "\n",
        "Θεωρώντας hallucinations τα datapoint με p(Hallucination) >= 0.66 προκύπτουν 29 πιθανά hallucinations απο τα οποία το 45.23% είναι σωστά"
      ],
      "metadata": {
        "id": "noSAF_fz4iXl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Για το αρχείο processed_df.json έχουμε 56.25% accuracy με 37 predicted hallucinations (p(Hallucination) = 1)\n",
        "\n",
        "Για p(Hallucination) >= 0.66 47.61% accuracy με 33 predicted hallucinations"
      ],
      "metadata": {
        "id": "8FNMF_rwxpDz"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sYFey8HsY2lq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "20ce6439-af74-4d98-9f24-72999e9a9eb9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting sentence-transformers\n",
            "  Downloading sentence-transformers-2.2.2.tar.gz (85 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.0/86.0 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting transformers<5.0.0,>=4.6.0 (from sentence-transformers)\n",
            "  Downloading transformers-4.34.1-py3-none-any.whl (7.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.7/7.7 MB\u001b[0m \u001b[31m31.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (4.66.1)\n",
            "Requirement already satisfied: torch>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (2.1.0+cu118)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (0.16.0+cu118)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (1.23.5)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (1.2.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (1.11.3)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (3.8.1)\n",
            "Collecting sentencepiece (from sentence-transformers)\n",
            "  Downloading sentencepiece-0.1.99-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m55.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting huggingface-hub>=0.4.0 (from sentence-transformers)\n",
            "  Downloading huggingface_hub-0.18.0-py3-none-any.whl (301 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m302.0/302.0 kB\u001b[0m \u001b[31m31.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers) (3.12.4)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers) (2023.6.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers) (2.31.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers) (6.0.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers) (4.5.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers) (23.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->sentence-transformers) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->sentence-transformers) (3.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->sentence-transformers) (3.1.2)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->sentence-transformers) (2.1.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (2023.6.3)\n",
            "Collecting tokenizers<0.15,>=0.14 (from transformers<5.0.0,>=4.6.0->sentence-transformers)\n",
            "  Downloading tokenizers-0.14.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.8/3.8 MB\u001b[0m \u001b[31m77.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting safetensors>=0.3.1 (from transformers<5.0.0,>=4.6.0->sentence-transformers)\n",
            "  Downloading safetensors-0.4.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m72.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk->sentence-transformers) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk->sentence-transformers) (1.3.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence-transformers) (3.2.0)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision->sentence-transformers) (9.4.0)\n",
            "Collecting huggingface-hub>=0.4.0 (from sentence-transformers)\n",
            "  Downloading huggingface_hub-0.17.3-py3-none-any.whl (295 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m295.0/295.0 kB\u001b[0m \u001b[31m33.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.6.0->sentence-transformers) (2.1.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers) (3.3.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers) (2023.7.22)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.6.0->sentence-transformers) (1.3.0)\n",
            "Building wheels for collected packages: sentence-transformers\n",
            "  Building wheel for sentence-transformers (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sentence-transformers: filename=sentence_transformers-2.2.2-py3-none-any.whl size=125923 sha256=17b46b30e4d79397c9994903331b0414ae02d0e3210caa700e56a1796d394f70\n",
            "  Stored in directory: /root/.cache/pip/wheels/62/f2/10/1e606fd5f02395388f74e7462910fe851042f97238cbbd902f\n",
            "Successfully built sentence-transformers\n",
            "Installing collected packages: sentencepiece, safetensors, huggingface-hub, tokenizers, transformers, sentence-transformers\n",
            "Successfully installed huggingface-hub-0.17.3 safetensors-0.4.0 sentence-transformers-2.2.2 sentencepiece-0.1.99 tokenizers-0.14.1 transformers-4.34.1\n"
          ]
        }
      ],
      "source": [
        "pip install -U sentence-transformers"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sentence_transformers import SentenceTransformer\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import json\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from collections import Counter"
      ],
      "metadata": {
        "id": "aXrCR3apY9X6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model  = SentenceTransformer(\"bert-large-cased-whole-word-masking-finetuned-squad\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kqscfruNZDHn",
        "outputId": "45211409-1583-417b-b3f6-28d25ea5c18a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:sentence_transformers.SentenceTransformer:No sentence-transformers model found with name /root/.cache/torch/sentence_transformers/bert-large-cased-whole-word-masking-finetuned-squad. Creating a new one with MEAN pooling.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with open(\"pg_processed_df.json\", \"r\") as f:\n",
        "    trial_data = json.load(f)\n",
        "\n",
        "\n",
        "df = pd.json_normalize(trial_data)\n",
        "\n",
        "#df.head(79)"
      ],
      "metadata": {
        "id": "pSdGc6yrZTwZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "result_dict = {}\n",
        "hallucination_cosines_total = 0\n",
        "hallucinations_cosines = []\n",
        "hallucinations = []\n",
        "cosine_similarities = []\n",
        "\n",
        "for i in range(0, len(df)):\n",
        "    sentences = [df.iloc[i]['hyp'], df.iloc[i]['tgt']]\n",
        "    embeddings = model.encode(sentences) #tokenization is implicitly handled by the SentenceTransformer here\n",
        "    similarity = cosine_similarity(embeddings[0:1], embeddings[1:2])\n",
        "\n",
        "    for id_h, h in enumerate(sentences):\n",
        "        for id_t, t in enumerate(sentences):\n",
        "            if id_h >= id_t:\n",
        "                continue\n",
        "            dp_id = len(result_dict)  #Using dp_id as the key\n",
        "            #if df.iloc[i]['p(Hallucination)'] >= 0.66:\n",
        "            if df.iloc[i]['p(Hallucination)'] == 1:\n",
        "                hallucinations.append(dp_id)\n",
        "                hallucination_cosines_total += similarity[0][0]\n",
        "                hallucinations_cosines.append(similarity[0][0])\n",
        "\n",
        "            result_dict[dp_id] = {'hyp': h, 'tgt': t, 'cosine_similarity': similarity[0][0]}\n",
        "\n",
        "\n",
        "for dp_id, values in result_dict.items():\n",
        "    cosine_similarities.append(values['cosine_similarity'])\n",
        "    print(f'Datapoint {dp_id}:')\n",
        "    print(f'hyp: {values[\"hyp\"]}')\n",
        "    print(f'tgt: {values[\"tgt\"]}')\n",
        "    print(f'cosine similarity: {values[\"cosine_similarity\"]}')\n",
        "    print()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bHjVoLxGhj1F",
        "outputId": "d02c6dc5-254d-4415-cdc6-12dcc6d4c748"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Datapoint 0:\n",
            "hyp: that’s happening moment\n",
            "tgt: isn’t gonna happen\n",
            "cosine similarity: 0.8340305685997009\n",
            "\n",
            "Datapoint 1:\n",
            "hyp: permit carry\n",
            "tgt: uh i’m validated\n",
            "cosine similarity: 0.795008659362793\n",
            "\n",
            "Datapoint 2:\n",
            "hyp: process easy\n",
            "tgt: watch\n",
            "cosine similarity: 0.77921062707901\n",
            "\n",
            "Datapoint 3:\n",
            "hyp: number five eight\n",
            "tgt: 5 6 7 8\n",
            "cosine similarity: 0.8790779113769531\n",
            "\n",
            "Datapoint 4:\n",
            "hyp: safer way travel\n",
            "tgt: it’s safer\n",
            "cosine similarity: 0.856874406337738\n",
            "\n",
            "Datapoint 5:\n",
            "hyp: you’re good scam artist\n",
            "tgt: imposter\n",
            "cosine similarity: 0.8198150396347046\n",
            "\n",
            "Datapoint 6:\n",
            "hyp: see\n",
            "tgt: last time saw\n",
            "cosine similarity: 0.7504163980484009\n",
            "\n",
            "Datapoint 7:\n",
            "hyp: let leave\n",
            "tgt: someone let\n",
            "cosine similarity: 0.7251225709915161\n",
            "\n",
            "Datapoint 8:\n",
            "hyp: impossible\n",
            "tgt: there’s nothing can’t done\n",
            "cosine similarity: 0.7050991058349609\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "threshold = np.mean(hallucinations_cosines)\n",
        "\n",
        "hallucinations_pred = []\n",
        "\n",
        "# Iterate through result_dict and apply the thresholding logic\n",
        "for dp_id, values in result_dict.items():\n",
        "    similarity = values['cosine_similarity']\n",
        "    if similarity < threshold:\n",
        "        hallucinations_pred.append(dp_id)\n",
        "\n",
        "counter1 = Counter(hallucinations)\n",
        "counter2 = Counter(hallucinations_pred)\n",
        "\n",
        "intersection = counter1 & counter2\n",
        "\n",
        "total_mutual_elements = sum(intersection.values())\n",
        "\n",
        "print(f'Threshold: {threshold}')\n",
        "print(f'Total Hallucinations: {len(hallucinations)}')\n",
        "print(f'Total Hallucinations Predicted: {len(hallucinations_pred)}')\n",
        "print('Percentage of correctly predicted hallucinations:' , (total_mutual_elements / len(hallucinations)) * 100 , '%')\n",
        "print('Correct Hallucinations:', intersection.keys())\n",
        "print('Number of correct Hallucinations:', len(intersection.keys()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iG0ziOztiUiW",
        "outputId": "42877368-4fac-4534-c8d8-72dc3840c0be"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Threshold: 0.8238994479179382\n",
            "Total Hallucinations: 3\n",
            "Total Hallucinations Predicted: 6\n",
            "Percentage of correctly predicted hallucinations: 66.66666666666666 %\n",
            "Correct Hallucinations: dict_keys([1, 5])\n",
            "Number of correct Hallucinations: 2\n"
          ]
        }
      ]
    }
  ]
}